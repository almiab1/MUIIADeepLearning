{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almiab1/MUIIADeepLearning/blob/main/PracticaFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PsNaYc9RvmD1"
      },
      "source": [
        "# Práctica Final | Deep Learning\n",
        "\n",
        "En el presente trabajo se proponen dos modelos basados en redes convolucionales, los cuales se trata de clasificar correctamente si un individuo está parpadeando o no. En la actualidad se pueden encontrar modelos de gran calidad, los cuales permiten construir modelos de gran precisión basándose sobre la base de dichos modelos pre-entrenados. Por ello, los modelos propuestos en este trabajo se definen haciendo uso de modelos previamente entrenados.\n",
        "\n",
        "En primer lugar, se propone una red basada en el modelo `InceptionResNetV2`. Un modelo pesado de gran profundidad que proporciona buenos resultados en problemas de clasificación. Se propone una estrategia donde el modelo `InceptionResNetV2` toma la función de extractor de características, obteniendo así las características profundas que posteriormente podrán ser utilizadas en la clasificación de las imágenes. En segundo punto, se propone una red basada en el modelo `DenseNet`. Un modelo de menor tamaño y que permite obtener resultados similares a los de `InceptionResNetV2`. En este caso se trata de re-entrenar el clasificador y aplicando `fine tuning` con el fin de ajustar los parámetros del modelo.\n",
        "\n",
        "Dado que se parte de un conjunto de datos desbalanceado, es necesario aplicar algunos métodos de balanceo con el fin de obtener un modelo más robusto. Previamente, al entrenamiento de la red, se propone el uso de técnicas como la estratificación de datos o `data augmentation`. A su vez, para mejorar el proceso de entrenamiento se hace empleo de `mini-batches` y de optimizadores como el optimizador Adam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DLNUw-vuvhRx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aijutic/miniconda3/envs/cnnDL/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.12.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow_addons.metrics import F1Score \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Keras imports\n",
        "from tensorflow.keras import optimizers, Input\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Concatenate, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.applications import InceptionResNetV2, DenseNet169 \n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-MWan3KCXmG",
        "outputId": "8f818470-36ed-4c52-f295-a07b81e27e54"
      },
      "outputs": [],
      "source": [
        "_isColab = False\n",
        "\n",
        "if _isColab:\n",
        "    from google.colab import drive\n",
        "\n",
        "    # Montamos el Google Drive en el directorio del proyecto y descomprimios el fichero con los datos\n",
        "    drive.mount('/content/drive')\n",
        "    !unzip -n '/content/drive/MyDrive/UIMP/Asignaturas/5-DeepLearning/Practicas/Final/Source/RT-BENE.zip' >> /dev/null"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pasos previos\n",
        "\n",
        "### Preparar el conjunto de datos\n",
        "\n",
        "*   **Cargar el dataset en el colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sOwv5P20IHLv",
        "outputId": "aeaee0a6-2d91-4217-e64a-5afb112a8a4d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>blink_id</th>\n",
              "      <th>left_eye</th>\n",
              "      <th>right_eye</th>\n",
              "      <th>video</th>\n",
              "      <th>blink</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0_left_000001_rgb.png</td>\n",
              "      <td>0_right_000001_rgb.png</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0_left_000002_rgb.png</td>\n",
              "      <td>0_right_000002_rgb.png</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0_left_000003_rgb.png</td>\n",
              "      <td>0_right_000003_rgb.png</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0_left_000004_rgb.png</td>\n",
              "      <td>0_right_000004_rgb.png</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0_left_000005_rgb.png</td>\n",
              "      <td>0_right_000005_rgb.png</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   blink_id               left_eye               right_eye  video blink\n",
              "0         0  0_left_000001_rgb.png  0_right_000001_rgb.png      0     0\n",
              "1         1  0_left_000002_rgb.png  0_right_000002_rgb.png      0     0\n",
              "2         2  0_left_000003_rgb.png  0_right_000003_rgb.png      0     0\n",
              "3         3  0_left_000004_rgb.png  0_right_000004_rgb.png      0     0\n",
              "4         4  0_left_000005_rgb.png  0_right_000005_rgb.png      0     0"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Especificamos los paths al directorio que contiene las imagenes y al fichero con las etiquetas\n",
        "data_path = 'RT-BENE/'\n",
        "\n",
        "if _isColab == False:\n",
        "    data_path = \"./../RT-BENE/\"\n",
        "\n",
        "imgs_path = data_path + \"images/\"\n",
        "labels_path = data_path + \"blinks.csv\"\n",
        "\n",
        "\n",
        "# Leemos el fichero CSV con las etiquetas\n",
        "labels = pd.read_csv(labels_path, dtype = {\"class\": \"category\"})\n",
        "\n",
        "# Supongamos que tienes un DataFrame llamado 'labels' con los nombres de las imágenes y las etiquetas 'blink'\n",
        "# Convertir la columna 'blink' a tipo string\n",
        "labels['blink'] = labels['blink'].astype(str)\n",
        "\n",
        "# Mostramos los primero elementos del dataset\n",
        "labels.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E7VPECAbLCyN"
      },
      "source": [
        "- **Creamos las tres particiones de datos: entrenamiento, validación y test**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8FXPaZ16Ig0Y"
      },
      "outputs": [],
      "source": [
        "# Semilla para replicar los experimentos\n",
        "seed = 2023\n",
        "# Creamos las tres particiones de datos: entrenamiento, validación y test\n",
        "train_data, test_data = train_test_split(labels, test_size=0.2, random_state=seed)\n",
        "dev_data, test_data = train_test_split(test_data, test_size=0.5, random_state=seed)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xwZjOHteLfAo"
      },
      "source": [
        "En Keras no existen generadores por defecto que devuelvan dos imágenes. Necesitamos crear nuestro propio generador que devuelva a la vez las imágenes de ambos ojos y la etiqueta del frame. Para ello podemos crear dos generadores (uno para cada ojo) usando el método `flow_from_databrame` y combinarlos para crear el generador deseado.\n",
        "\n",
        "- En primer lugar se hace uso del método `ImageDataGenerator` para preprocesar imágenes y generar lotes de datos de entrenamiento y validación.\n",
        "- De seguido se definen los generadores de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1tKJqLxZv836"
      },
      "outputs": [],
      "source": [
        "# Aplicar tecnicas de preprocesado\n",
        "datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, # rotación aleatoria en un rango de 20 grados\n",
        "    width_shift_range=0.1, # desplazamiento horizontal aleatorio en un rango del 10% de la anchura de la imagen\n",
        "    height_shift_range=0.1, # desplazamiento vertical aleatorio en un rango del 10% de la altura de la imagen\n",
        "    zoom_range=0.1, # zoom aleatorio en un rango del 10%\n",
        "    horizontal_flip=True, # reflejo horizontal aleatorio\n",
        "    vertical_flip=False) # reflejo horizontal aleatorio\n",
        "\n",
        "# Columnas dataset\n",
        "left_eye_col = 'left_eye'\n",
        "right_eye_col = 'right_eye'\n",
        "y_col = 'blink'\n",
        "\n",
        "# Parámetros\n",
        "batch_size = 128\n",
        "img_width = 122 # minimo 75x75\n",
        "img_height = 75\n",
        "\n",
        "# Generador custom que devuelve las dos imagenes de ojos y el label del parpadeo\n",
        "def generator(dataframe):\n",
        "  left_eye_generator = datagen.flow_from_dataframe(dataframe=dataframe, \n",
        "                                                    directory = imgs_path, \n",
        "                                                    target_size =(img_width, img_height), \n",
        "                                                    x_col=left_eye_col, \n",
        "                                                    y_col=y_col, \n",
        "                                                    class_mode=\"binary\", \n",
        "                                                    seed=seed, \n",
        "                                                    batch_size=batch_size)\n",
        "  \n",
        "  right_eye_generator = datagen.flow_from_dataframe(dataframe=dataframe,\n",
        "                                                    directory = imgs_path,\n",
        "                                                    target_size =(img_width, img_height),\n",
        "                                                    x_col=right_eye_col,\n",
        "                                                    y_col=y_col,\n",
        "                                                    class_mode=\"binary\",\n",
        "                                                    seed=seed,\n",
        "                                                    batch_size=batch_size)\n",
        "  \n",
        "  while True:\n",
        "    left_eye = left_eye_generator.next()\n",
        "    left_eye_image = left_eye[0]\n",
        "    label = left_eye[1]\n",
        "    right_eye = right_eye_generator.next()\n",
        "    right_eye_image = right_eye[0]\n",
        "    yield [left_eye_image, right_eye_image], label\n",
        "\n",
        "# Llamada a la función generator\n",
        "train_generator = generator(train_data)\n",
        "dev_generator = generator(dev_data)\n",
        "test_generator = generator(test_data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Definición de parámetros comunes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metal device set to: Apple M1 Ultra\n",
            "\n",
            "systemMemory: 64.00 GB\n",
            "maxCacheSize: 24.00 GB\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\" Configuraciones previas \"\"\"\n",
        "\n",
        "# Definimos la métrica F1\n",
        "f1_score = F1Score(name=\"f1_score\", num_classes=1, threshold=0.5, average='macro')\n",
        "\n",
        "# Declaramos dos capas de Input\n",
        "input_shape = (img_width,img_height,3)\n",
        "\n",
        "input_image1 = Input(shape=input_shape)\n",
        "input_image2 = Input(shape=input_shape)\n",
        "\n",
        "# Definimos el ratio de aprendizaje\n",
        "lr = 1e-4"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trasnfer learing con IceptionResNetV2 como extractor de características"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes de entrenar el modelo, es importante decidir qué capas se actualizarán durante el entrenamiento. Si deseas aplicar fine-tuning en algunas de las capas del modelo base, primero puedes congelar todas las capas y luego descongelar las últimas capas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XZn-QbmRI8Q",
        "outputId": "8af6f2a2-8e83-494b-ba6e-b5fdc039aec4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
            "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 122, 75, 3)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 122, 75, 3)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " inception_resnet_v2 (Functiona  (None, 1536)        54336736    ['input_1[0][0]',                \n",
            " l)                                                               'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 3072)         0           ['inception_resnet_v2[0][0]',    \n",
            "                                                                  'inception_resnet_v2[1][0]']    \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 1024)         3146752     ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 1024)         0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 512)          524800      ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 1)            513         ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 58,008,801\n",
            "Trainable params: 3,672,065\n",
            "Non-trainable params: 54,336,736\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Cargar InceptionResNetV2 pre-entrenado en ImageNet, sin la capa final\n",
        "base_model_incept = InceptionResNetV2(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "# Congelar todas las capas del modelo base para aplicar transfer learning\n",
        "for layer in base_model_incept.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Extraer las características utilizando InceptionResNetV2\n",
        "features_image1 = base_model_incept(input_image1)\n",
        "features_image2 = base_model_incept(input_image2)\n",
        "\n",
        "# Combinar las características de ambas imágenes\n",
        "merged_features = Concatenate()([features_image1, features_image2])\n",
        "\n",
        "# Añadir capas adicionales para la clasificación\n",
        "x = Dense(1024, activation='relu')(merged_features)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Crear el modelo\n",
        "model_incept = Model(inputs=[input_image1, input_image2], outputs=output)\n",
        "\n",
        "# Compilar el modelo\n",
        "model_incept.compile(optimizer=optimizers.Adam(learning_rate=lr), loss='binary_crossentropy', metrics=['accuracy', f1_score])\n",
        "\n",
        "# Imprimir un resumen del modelo\n",
        "model_incept.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB-EZ9e5SUiR",
        "outputId": "03c452e9-1865-4b61-e3ea-25efda3f0137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 85880 validated image filenames belonging to 2 classes.\n",
            "Found 85880 validated image filenames belonging to 2 classes.\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-05 17:14:32.448808: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "670/670 - 110s - loss: 0.0572 - accuracy: 0.9807 - f1_score: 0.7395 - val_loss: 0.0282 - val_accuracy: 0.9905 - val_f1_score: 0.8789 - 110s/epoch - 164ms/step\n",
            "Epoch 2/10\n",
            "670/670 - 95s - loss: 0.0310 - accuracy: 0.9897 - f1_score: 0.8715 - val_loss: 0.0216 - val_accuracy: 0.9924 - val_f1_score: 0.9021 - 95s/epoch - 141ms/step\n",
            "Epoch 3/10\n",
            "670/670 - 96s - loss: 0.0241 - accuracy: 0.9917 - f1_score: 0.9004 - val_loss: 0.0207 - val_accuracy: 0.9927 - val_f1_score: 0.9095 - 96s/epoch - 143ms/step\n",
            "Epoch 4/10\n",
            "670/670 - 95s - loss: 0.0213 - accuracy: 0.9929 - f1_score: 0.9114 - val_loss: 0.0172 - val_accuracy: 0.9946 - val_f1_score: 0.9316 - 95s/epoch - 142ms/step\n",
            "Epoch 5/10\n",
            "670/670 - 95s - loss: 0.0184 - accuracy: 0.9937 - f1_score: 0.9250 - val_loss: 0.0185 - val_accuracy: 0.9943 - val_f1_score: 0.9271 - 95s/epoch - 141ms/step\n",
            "Epoch 6/10\n",
            "670/670 - 94s - loss: 0.0167 - accuracy: 0.9942 - f1_score: 0.9307 - val_loss: 0.0164 - val_accuracy: 0.9947 - val_f1_score: 0.9319 - 94s/epoch - 140ms/step\n",
            "Epoch 7/10\n",
            "670/670 - 95s - loss: 0.0163 - accuracy: 0.9947 - f1_score: 0.9382 - val_loss: 0.0101 - val_accuracy: 0.9969 - val_f1_score: 0.9591 - 95s/epoch - 142ms/step\n",
            "Epoch 8/10\n",
            "670/670 - 102s - loss: 0.0133 - accuracy: 0.9957 - f1_score: 0.9473 - val_loss: 0.0071 - val_accuracy: 0.9972 - val_f1_score: 0.9672 - 102s/epoch - 152ms/step\n",
            "Epoch 9/10\n",
            "670/670 - 98s - loss: 0.0124 - accuracy: 0.9958 - f1_score: 0.9495 - val_loss: 0.0062 - val_accuracy: 0.9977 - val_f1_score: 0.9735 - 98s/epoch - 146ms/step\n",
            "Epoch 10/10\n",
            "670/670 - 96s - loss: 0.0114 - accuracy: 0.9961 - f1_score: 0.9526 - val_loss: 0.0081 - val_accuracy: 0.9978 - val_f1_score: 0.9725 - 96s/epoch - 144ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x3513fafb0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Configuración del método 'Early stopping'\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Entrenamos el modelo con los datos preparados en el punto 2\n",
        "model_incept.fit(train_generator,\n",
        "          epochs=10,  # numero de epochs\n",
        "          verbose=2,  # muestra informacion del error al finalizar cada epoch\n",
        "          steps_per_epoch=len(train_data)/batch_size,\n",
        "          validation_data=train_generator,\n",
        "          validation_steps=len(dev_data)/batch_size,\n",
        "          callbacks=[early_stopping]) # Añadir el callback de 'early stoppoing'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Después del entrenamiento, evalúa el rendimiento del modelo en el conjunto de prueba:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Found 85880 validated image filenames belonging to 2 classes.\n",
            "Found 85880 validated image filenames belonging to 2 classes.\n",
            "83/83 [==============================] - 11s 129ms/step - loss: 0.0071 - accuracy: 0.9978 - f1_score: 0.9729\n",
            "Test_loss: 0.0071, Test_acc: 0.9978, Test_F1_Score: 0.9729\n"
          ]
        }
      ],
      "source": [
        "# Por ultimo, podemos evaluar el modelo en el conjunto de test\n",
        "test_loss, test_acc, test_f1_score = model_incept.evaluate(test_generator, steps=len(test_data) / batch_size, verbose=1)\n",
        "print(f\"Test_loss: {test_loss:.4f}, Test_acc: {test_acc:.4f}, Test_F1_Score: {test_f1_score:.4f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Además de analizar el error obtenido, podemos utilizarlo para hacer predicciones. Para ello utilizaremos el método `predict`, al que le suministremos los datos de entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "7TMOFXPXE3YE",
        "outputId": "9d1a7e44-97a9-4324-d79f-407c2a18dfe5"
      },
      "outputs": [],
      "source": [
        "# Obtenemos las predicciones para todos los ejemplos del conjunto de test\n",
        "predictions = model_incept.predict(test_generator, verbose=1)\n",
        "\n",
        "# Imprimimos la predicción obtenida para los dos primeros ejemplos\n",
        "# Los valores obtenidos representan las probabilidades para cada una de las 5 clases\n",
        "for i in range(0,2):\n",
        "  print(\"\\n Ejemplo\", i)\n",
        "  print(\"\\t Probabilidades para las 5 clases:\", predictions[i])\n",
        "  print(\"\\t Clase predicha: %i, Probabilidad: %.4f\" % (np.argmax([predictions[i]]), np.max(predictions[i])))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine Tuning con DenseNet"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este caso se hace uso del modelo **DenseNet121**. Este tiene un tamaño de 80 MB, ha sido entrenado con 20.1 millones de parámetros y tiene una profuncidad de 402 capas. Respecto a la precisión del mismo, presenta un 93.6% de precisión para el dataset de validación de ImageNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "51877672/51877672 [==============================] - 3s 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
            "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        }
      ],
      "source": [
        "# Cargar el modelo DenseNet121 pre-entrenado en ImageNet sin incluir la capa de salida\n",
        "base_model_densenet = DenseNet169(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
        "\n",
        "# Congelar todas las capas del modelo base\n",
        "for layer in base_model_densenet.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Descongelar las últimas capas para aplicar fine-tuning\n",
        "# (en este ejemplo, descongelamos las últimas 50 capas)\n",
        "for layer in base_model_densenet.layers[-50:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Añadir una capa de GlobalAveragePooling2D\n",
        "x = base_model_densenet.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Añadir una capa Dense de salida con una función de activación sigmoide (clasificación binaria)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Crear el nuevo modelo\n",
        "model_dn = Model(inputs=[input_image1, input_image2], outputs=output)\n",
        "\n",
        "# Compilar el modelo con la función de pérdida binary_crossentropy y la métrica de precisión\n",
        "model_dn.compile(optimizer=optimizers.Adam(learning_rate=lr), loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Entrenar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "670/670 - 175s - loss: 0.0099 - accuracy: 0.9966 - f1_score: 0.9598 - val_loss: 0.0055 - val_accuracy: 0.9985 - val_f1_score: 0.9831 - 175s/epoch - 261ms/step\n",
            "Epoch 2/10\n",
            "670/670 - 140s - loss: 0.0105 - accuracy: 0.9964 - f1_score: 0.9567 - val_loss: 0.0136 - val_accuracy: 0.9948 - val_f1_score: 0.9372 - 140s/epoch - 209ms/step\n",
            "Epoch 3/10\n",
            "670/670 - 140s - loss: 0.0099 - accuracy: 0.9966 - f1_score: 0.9604 - val_loss: 0.0044 - val_accuracy: 0.9986 - val_f1_score: 0.9829 - 140s/epoch - 208ms/step\n",
            "Epoch 4/10\n",
            "670/670 - 132s - loss: 0.0084 - accuracy: 0.9970 - f1_score: 0.9637 - val_loss: 0.0067 - val_accuracy: 0.9977 - val_f1_score: 0.9730 - 132s/epoch - 196ms/step\n",
            "Epoch 5/10\n",
            "670/670 - 135s - loss: 0.0082 - accuracy: 0.9972 - f1_score: 0.9660 - val_loss: 0.0048 - val_accuracy: 0.9981 - val_f1_score: 0.9764 - 135s/epoch - 201ms/step\n",
            "Epoch 6/10\n",
            "670/670 - 139s - loss: 0.0083 - accuracy: 0.9972 - f1_score: 0.9661 - val_loss: 0.0054 - val_accuracy: 0.9980 - val_f1_score: 0.9761 - 139s/epoch - 208ms/step\n",
            "Epoch 7/10\n",
            "670/670 - 142s - loss: 0.0070 - accuracy: 0.9975 - f1_score: 0.9700 - val_loss: 0.0038 - val_accuracy: 0.9990 - val_f1_score: 0.9879 - 142s/epoch - 212ms/step\n",
            "Epoch 8/10\n",
            "670/670 - 142s - loss: 0.0071 - accuracy: 0.9974 - f1_score: 0.9694 - val_loss: 0.0036 - val_accuracy: 0.9989 - val_f1_score: 0.9869 - 142s/epoch - 211ms/step\n",
            "Epoch 9/10\n",
            "670/670 - 138s - loss: 0.0068 - accuracy: 0.9977 - f1_score: 0.9717 - val_loss: 0.0041 - val_accuracy: 0.9985 - val_f1_score: 0.9826 - 138s/epoch - 206ms/step\n",
            "Epoch 10/10\n",
            "670/670 - 144s - loss: 0.0056 - accuracy: 0.9981 - f1_score: 0.9769 - val_loss: 0.0029 - val_accuracy: 0.9989 - val_f1_score: 0.9876 - 144s/epoch - 215ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x662a9bd90>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Configuración del método 'Early stopping'\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Entrenamos el modelo con los datos preparados en el punto 2\n",
        "model_dn.fit(train_generator,\n",
        "          epochs=10,  # numero de epochs\n",
        "          verbose=2,  # muestra informacion del error al finalizar cada epoch\n",
        "          steps_per_epoch=len(train_data)/batch_size,\n",
        "          validation_data=train_generator,\n",
        "          validation_steps=len(dev_data)/batch_size,\n",
        "          callbacks=[early_stopping]) # Añadir el callback de 'early stoppoing'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Después del entrenamiento, evalúa el rendimiento del modelo en el conjunto de prueba:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/Users/aijutic/miniconda3/envs/cnnDL/lib/python3.10/site-packages/keras/engine/training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"/Users/aijutic/miniconda3/envs/cnnDL/lib/python3.10/site-packages/keras/engine/training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/aijutic/miniconda3/envs/cnnDL/lib/python3.10/site-packages/keras/engine/training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"/Users/aijutic/miniconda3/envs/cnnDL/lib/python3.10/site-packages/keras/engine/training.py\", line 1788, in test_step\n        y_pred = self(x, training=False)\n    File \"/Users/aijutic/miniconda3/envs/cnnDL/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/aijutic/miniconda3/envs/cnnDL/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_1\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, None, None, None) dtype=float32>]\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Por ultimo, podemos evaluar el modelo en el conjunto de test\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m()\n\u001b[0;32m----> 3\u001b[0m test_loss, test_acc, test_f1_score \u001b[39m=\u001b[39m model_dn\u001b[39m.\u001b[39;49mevaluate(test_generator, steps\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(test_data) \u001b[39m/\u001b[39;49m batch_size, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest_loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Test_acc: \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Test_F1_Score: \u001b[39m\u001b[39m{\u001b[39;00mtest_f1_score\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/cnnDL/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m/var/folders/7b/vkn97zrd41j1d9vfyz756_t80000gn/T/__autograph_generated_file28k0gu1l.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/aijutic/miniconda3/envs/cnnDL/lib/python3.10/site-packages/keras/engine/training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"/Users/aijutic/miniconda3/envs/cnnDL/lib/python3.10/site-packages/keras/engine/training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/aijutic/miniconda3/envs/cnnDL/lib/python3.10/site-packages/keras/engine/training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"/Users/aijutic/miniconda3/envs/cnnDL/lib/python3.10/site-packages/keras/engine/training.py\", line 1788, in test_step\n        y_pred = self(x, training=False)\n    File \"/Users/aijutic/miniconda3/envs/cnnDL/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/aijutic/miniconda3/envs/cnnDL/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_1\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, None, None, None) dtype=float32>]\n"
          ]
        }
      ],
      "source": [
        "# Por ultimo, podemos evaluar el modelo en el conjunto de test\n",
        "test_loss, test_acc, test_f1_score = model_dn.evaluate(test_generator, steps=len(test_data) / batch_size, verbose=1)\n",
        "print(f\"Test_loss: {test_loss:.4f}, Test_acc: {test_acc:.4f}, Test_F1_Score: {test_f1_score:.4f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Además de analizar el error obtenido, podemos utilizarlo para hacer predicciones. Para ello utilizaremos el método `predict`, al que le suministremos los datos de entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtenemos las predicciones para todos los ejemplos del conjunto de test\n",
        "predictions = model_incept.predict(test_generator, verbose=1)\n",
        "\n",
        "# Imprimimos la predicción obtenida para los dos primeros ejemplos\n",
        "# Los valores obtenidos representan las probabilidades para cada una de las 5 clases\n",
        "for i in range(0,2):\n",
        "  print(\"\\n Ejemplo\", i)\n",
        "  print(\"\\t Probabilidades para las 5 clases:\", predictions[i])\n",
        "  print(\"\\t Clase predicha: %i, Probabilidad: %.4f\" % (np.argmax([predictions[i]]), np.max(predictions[i])))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPYRKocSK9HLJdrEdQMa+Yt",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
