{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almiab1/MUIIADeepLearning/blob/main/PracticaFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsNaYc9RvmD1"
      },
      "source": [
        "# Práctica Final | Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiX4ktIbk29n"
      },
      "source": [
        "En el presente trabajo se proponen dos modelos basados en redes convolucionales, los cuales se trata de clasificar correctamente si un individuo está parpadeando o no. En la actualidad se pueden encontrar modelos de gran calidad, los cuales permiten construir modelos de gran precisión basándose sobre la base de dichos modelos pre-entrenados. Por ello, los modelos propuestos en este trabajo se definen haciendo uso de modelos previamente entrenados.\n",
        "\n",
        "En primer lugar, se propone una red basada en el modelo `InceptionResNetV2`. Un modelo pesado de gran profundidad que proporciona buenos resultados en problemas de clasificación. Se propone una estrategia donde el modelo `InceptionResNetV2` toma la función de extractor de características, obteniendo así las características profundas que posteriormente podrán ser utilizadas en la clasificación de las imágenes. En segundo punto, se propone una red basada en el modelo `DenseNet`. Un modelo de menor tamaño y que permite obtener resultados similares a los de `InceptionResNetV2`. En este caso se trata de re-entrenar el clasificador y aplicando `fine tuning` con el fin de ajustar los parámetros del modelo.\n",
        "\n",
        "Dado que se parte de un conjunto de datos desbalanceado, es necesario aplicar algunos métodos de balanceo con el fin de obtener un modelo más robusto. Previamente, al entrenamiento de la red, se propone el uso de técnicas como la estratificación de datos o `data augmentation`. A su vez, para mejorar el proceso de entrenamiento se hace empleo de `mini-batches` y de optimizadores como el optimizador Adam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DLNUw-vuvhRx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras import applications, optimizers, Input\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.applications import InceptionResNetV2\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-MWan3KCXmG",
        "outputId": "8f818470-36ed-4c52-f295-a07b81e27e54"
      },
      "outputs": [],
      "source": [
        "_isColab = False\n",
        "\n",
        "if _isColab:\n",
        "    from google.colab import drive\n",
        "\n",
        "    # Montamos el Google Drive en el directorio del proyecto y descomprimios el fichero con los datos\n",
        "    drive.mount('/content/drive')\n",
        "    !unzip -n '/content/drive/MyDrive/UIMP/Asignaturas/5-DeepLearning/Practicas/Final/Source/RT-BENE.zip' >> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrJHUIkdv9RO"
      },
      "source": [
        "## Preparar datos de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeLGe15ZLHjX"
      },
      "source": [
        "*   **Cargar el dataset en el colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sOwv5P20IHLv",
        "outputId": "aeaee0a6-2d91-4217-e64a-5afb112a8a4d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>blink_id</th>\n",
              "      <th>left_eye</th>\n",
              "      <th>right_eye</th>\n",
              "      <th>video</th>\n",
              "      <th>blink</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0_left_000001_rgb.png</td>\n",
              "      <td>0_right_000001_rgb.png</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0_left_000002_rgb.png</td>\n",
              "      <td>0_right_000002_rgb.png</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0_left_000003_rgb.png</td>\n",
              "      <td>0_right_000003_rgb.png</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0_left_000004_rgb.png</td>\n",
              "      <td>0_right_000004_rgb.png</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0_left_000005_rgb.png</td>\n",
              "      <td>0_right_000005_rgb.png</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   blink_id               left_eye               right_eye  video blink\n",
              "0         0  0_left_000001_rgb.png  0_right_000001_rgb.png      0     0\n",
              "1         1  0_left_000002_rgb.png  0_right_000002_rgb.png      0     0\n",
              "2         2  0_left_000003_rgb.png  0_right_000003_rgb.png      0     0\n",
              "3         3  0_left_000004_rgb.png  0_right_000004_rgb.png      0     0\n",
              "4         4  0_left_000005_rgb.png  0_right_000005_rgb.png      0     0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Especificamos los paths al directorio que contiene las imagenes y al fichero con las etiquetas\n",
        "data_path = 'RT-BENE/'\n",
        "\n",
        "if _isColab == False:\n",
        "    data_path = \"./../RT-BENE/\"\n",
        "\n",
        "imgs_path = data_path + \"images/\"\n",
        "labels_path = data_path + \"blinks.csv\"\n",
        "\n",
        "\n",
        "# Leemos el fichero CSV con las etiquetas\n",
        "labels = pd.read_csv(labels_path, dtype = {\"class\": \"category\"})\n",
        "\n",
        "# Supongamos que tienes un DataFrame llamado 'labels' con los nombres de las imágenes y las etiquetas 'blink'\n",
        "# Convertir la columna 'blink' a tipo string\n",
        "labels['blink'] = labels['blink'].astype(str)\n",
        "\n",
        "# Mostramos los primero elementos del dataset\n",
        "labels.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7VPECAbLCyN"
      },
      "source": [
        "- **Creamos las tres particiones de datos: entrenamiento, validación y test**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8FXPaZ16Ig0Y"
      },
      "outputs": [],
      "source": [
        "# Semilla para replicar los experimentos\n",
        "seed = 2023\n",
        "# Creamos las tres particiones de datos: entrenamiento, validación y test\n",
        "train_data, test_data = train_test_split(labels, test_size=0.2, random_state=seed)\n",
        "dev_data, test_data = train_test_split(test_data, test_size=0.5, random_state=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwZjOHteLfAo"
      },
      "source": [
        "En Keras no existen generadores por defecto que devuelvan dos im\u0013agenes. Necesitamos crear nuestro propio generador que devuelva a la vez las imágenes de ambos ojos y la etiqueta del frame. Para ello podemos crear dos generadores (uno para cada ojo) usando el método `flow_from_databrame` y combinarlos para crear el generador deseado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1tKJqLxZv836"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Columnas dataset\n",
        "left_eye_col = 'left_eye'\n",
        "right_eye_col = 'right_eye'\n",
        "y_col = 'blink'\n",
        "\n",
        "# Parámetros\n",
        "batch_size = 128\n",
        "img_width = 122 # minimo 75x75\n",
        "img_height = 75\n",
        "\n",
        "# Generador custom que devuelve las dos imagenes de ojos y el label del parpadeo\n",
        "def generator(dataframe):\n",
        "  left_eye_generator = datagen.flow_from_dataframe(dataframe=dataframe, \n",
        "                                                    directory = imgs_path, \n",
        "                                                    target_size =(img_width, img_height), \n",
        "                                                    x_col=left_eye_col, \n",
        "                                                    y_col=y_col, \n",
        "                                                    class_mode=\"binary\", \n",
        "                                                    seed=seed, \n",
        "                                                    batch_size=batch_size)\n",
        "  \n",
        "  right_eye_generator = datagen.flow_from_dataframe(dataframe=dataframe,\n",
        "                                                    directory = imgs_path,\n",
        "                                                    target_size =(img_width, img_height),\n",
        "                                                    x_col=right_eye_col,\n",
        "                                                    y_col=y_col,\n",
        "                                                    class_mode=\"binary\",\n",
        "                                                    seed=seed,\n",
        "                                                    batch_size=batch_size)\n",
        "  \n",
        "  while True:\n",
        "    left_eye = left_eye_generator.next()\n",
        "    left_eye_image = left_eye[0]\n",
        "    label = left_eye[1]\n",
        "    right_eye = right_eye_generator.next()\n",
        "    right_eye_image = right_eye[0]\n",
        "    yield [left_eye_image, right_eye_image], label\n",
        "\n",
        "# Llamada a la función generator\n",
        "train_generator = generator(train_data)\n",
        "dev_generator = generator(train_data)\n",
        "test_generator = generator(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Jer5o8LzLzm7"
      },
      "outputs": [],
      "source": [
        "input_shape = (img_width,img_height,3)\n",
        "\n",
        "# Declaramos dos capas de Input\n",
        "input_image1 = Input(shape=input_shape)\n",
        "input_image2 = Input(shape=input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XZn-QbmRI8Q",
        "outputId": "8af6f2a2-8e83-494b-ba6e-b5fdc039aec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metal device set to: Apple M1 Ultra\n",
            "\n",
            "systemMemory: 64.00 GB\n",
            "maxCacheSize: 24.00 GB\n",
            "\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219055592/219055592 [==============================] - 12s 0us/step\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 122, 75, 3)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 122, 75, 3)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " inception_resnet_v2 (Functiona  (None, 1536)        54336736    ['input_1[0][0]',                \n",
            " l)                                                               'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 3072)         0           ['inception_resnet_v2[0][0]',    \n",
            "                                                                  'inception_resnet_v2[1][0]']    \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 1024)         3146752     ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 1024)         0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 512)          524800      ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 1)            513         ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 58,008,801\n",
            "Trainable params: 3,672,065\n",
            "Non-trainable params: 54,336,736\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Cargar InceptionResNetV2 pre-entrenado en ImageNet, sin la capa final\n",
        "base_model = InceptionResNetV2(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "# Congelar todas las capas del modelo base para aplicar transfer learning\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Extraer las características utilizando InceptionResNetV2\n",
        "features_image1 = base_model(input_image1)\n",
        "features_image2 = base_model(input_image2)\n",
        "\n",
        "# Combinar las características de ambas imágenes\n",
        "merged_features = Concatenate()([features_image1, features_image2])\n",
        "\n",
        "# Añadir capas adicionales para la clasificación\n",
        "x = Dense(1024, activation='relu')(merged_features)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Crear el modelo\n",
        "model = Model(inputs=[input_image1, input_image2], outputs=output)\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Imprimir un resumen del modelo\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB-EZ9e5SUiR",
        "outputId": "03c452e9-1865-4b61-e3ea-25efda3f0137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 85880 validated image filenames belonging to 2 classes.\n",
            "Found 85880 validated image filenames belonging to 2 classes.\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-05 14:32:12.879909: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "670/670 - 112s - loss: 0.0550 - accuracy: 0.9816 - val_loss: 0.0299 - val_accuracy: 0.9907 - 112s/epoch - 168ms/step\n",
            "Epoch 2/10\n",
            "670/670 - 93s - loss: 0.0304 - accuracy: 0.9898 - val_loss: 0.0219 - val_accuracy: 0.9924 - 93s/epoch - 138ms/step\n",
            "Epoch 3/10\n",
            "670/670 - 92s - loss: 0.0250 - accuracy: 0.9917 - val_loss: 0.0207 - val_accuracy: 0.9931 - 92s/epoch - 137ms/step\n",
            "Epoch 4/10\n",
            "670/670 - 92s - loss: 0.0215 - accuracy: 0.9928 - val_loss: 0.0202 - val_accuracy: 0.9934 - 92s/epoch - 138ms/step\n",
            "Epoch 5/10\n",
            "670/670 - 92s - loss: 0.0186 - accuracy: 0.9940 - val_loss: 0.0195 - val_accuracy: 0.9937 - 92s/epoch - 137ms/step\n",
            "Epoch 6/10\n",
            "670/670 - 91s - loss: 0.0167 - accuracy: 0.9944 - val_loss: 0.0160 - val_accuracy: 0.9942 - 91s/epoch - 136ms/step\n",
            "Epoch 7/10\n",
            "670/670 - 91s - loss: 0.0145 - accuracy: 0.9950 - val_loss: 0.0111 - val_accuracy: 0.9961 - 91s/epoch - 136ms/step\n",
            "Epoch 8/10\n",
            "670/670 - 97s - loss: 0.0140 - accuracy: 0.9952 - val_loss: 0.0079 - val_accuracy: 0.9973 - 97s/epoch - 145ms/step\n",
            "Epoch 9/10\n",
            "670/670 - 91s - loss: 0.0126 - accuracy: 0.9955 - val_loss: 0.0065 - val_accuracy: 0.9976 - 91s/epoch - 136ms/step\n",
            "Epoch 10/10\n",
            "670/670 - 92s - loss: 0.0107 - accuracy: 0.9963 - val_loss: 0.0076 - val_accuracy: 0.9972 - 92s/epoch - 137ms/step\n",
            "\n",
            "Found 85880 validated image filenames belonging to 2 classes.\n",
            "Found 85880 validated image filenames belonging to 2 classes.\n",
            "83/83 [==============================] - 10s 122ms/step - loss: 0.0073 - accuracy: 0.9977\n",
            "test_loss: 0.0073, test_acc: 0.9977\n"
          ]
        }
      ],
      "source": [
        "# Entrenamos el modelo con los datos preparados en el punto 2\n",
        "model.fit(train_generator,\n",
        "          epochs=10,  # numero de epochs\n",
        "          verbose=2,  # muestra informacion del error al finalizar cada epoch\n",
        "          steps_per_epoch=len(train_data)/batch_size,\n",
        "          validation_data=train_generator,\n",
        "          validation_steps=len(dev_data)/batch_size)\n",
        "\n",
        "# Por ultimo, podemos evaluar el modelo en el conjunto de test\n",
        "print()\n",
        "test_loss, test_acc = model.evaluate(test_generator,\n",
        "                                     steps=len(test_data)/batch_size,\n",
        "                                     verbose=1)\n",
        "print(\"test_loss: %.4f, test_acc: %.4f\" % (test_loss, test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "7TMOFXPXE3YE",
        "outputId": "9d1a7e44-97a9-4324-d79f-407c2a18dfe5"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-191a61fa0d47>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Creamos el modelo final y lo compilamos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# representación en modo texto del modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 )\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         nodes, nodes_by_depth, layers, _ = _map_graph_network(\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                         raise ValueError(\n\u001b[0m\u001b[1;32m   1143\u001b[0m                             \u001b[0;34m\"Graph disconnected: cannot obtain value for \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                             \u001b[0;34mf'tensor {x} at layer \"{layer.name}\". '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 122, 75, 3), dtype=tf.float32, name='input_14'), name='input_14', description=\"created by layer 'input_14'\") at layer \"conv2d_609\". The following previous layers were accessed without issue: []"
          ]
        }
      ],
      "source": [
        "# # Cargamos el modelo InceptionV3 pre-entrenado con ImageNet\n",
        "# base_model = applications.InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(img_width,img_height,3))\n",
        "\n",
        "# # Se va a ajustar los parámetros de las nuevas capas del modelo,\n",
        "# # dejando fijos los parámetros del resto de capa (es decir, que no se actualicen durante el entrenamiento)\n",
        "# for layer in base_model.layers:\n",
        "#     layer.trainable = False   # por defecto, el valor de trainable es True\n",
        "\n",
        "# # Añadimos nuevas capas al final para adaptar el modelo a nuestro problema\n",
        "# x = base_model.output\n",
        "# x = GlobalAveragePooling2D()(x)\n",
        "# x = Dense(1024, activation='relu')(x)\n",
        "\n",
        "# # Añadimos una última capa completamente conectada con 2 neuronas (número de clases) para obtener la salida de la red\n",
        "# predictions = Dense(2, activation='softmax')(x) \n",
        "\n",
        "# # Creamos el modelo final y lo compilamos\n",
        "# model = Model(inputs=[input_1, input_2], outputs=[predictions])\n",
        "# model.summary()   # representación en modo texto del modelo\n",
        "\n",
        "# model.compile(loss='categorical_crossentropy',                # función de pérdida para problemas de clasificación multi-clase\n",
        "#               optimizer=optimizers.Adam(learning_rate=1e-5),  # optimizador Adam\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# # Entrenamos el modelo con los datos preparados en el punto 2\n",
        "# model.fit(train_generator,\n",
        "#           epochs=10,  # numero de epochs\n",
        "#           verbose=2,  # muestra informacion del error al finalizar cada epoch\n",
        "#           steps_per_epoch=len(train_data)/batch_size,\n",
        "#           validation_data=train_generator,\n",
        "#           validation_steps=len(dev_data)/batch_size)\n",
        "\n",
        "# # Por ultimo, podemos evaluar el modelo en el conjunto de test\n",
        "# print()\n",
        "# test_loss, test_acc = model.evaluate(test_generator,\n",
        "#                                      steps=len(test_data)/batch_size,\n",
        "#                                      verbose=1)\n",
        "# print(\"test_loss: %.4f, test_acc: %.4f\" % (test_loss, test_acc))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPYRKocSK9HLJdrEdQMa+Yt",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
