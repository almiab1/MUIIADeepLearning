{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almiab1/MUIIADeepLearning/blob/main/PracticaFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsNaYc9RvmD1"
      },
      "source": [
        "# Práctica Final | Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiX4ktIbk29n"
      },
      "source": [
        "En el presente trabajo se proponen dos modelos basados en redes convolucionales, los cuales se trata de clasificar correctamente si un individuo está parpadeando o no. En la actualidad se pueden encontrar modelos de gran calidad, los cuales permiten construir modelos de gran precisión basándose sobre la base de dichos modelos pre-entrenados. Por ello, los modelos propuestos en este trabajo se definen haciendo uso de modelos previamente entrenados.\n",
        "\n",
        "En primer lugar, se propone una red basada en el modelo `InceptionResNetV2`. Un modelo pesado de gran profundidad que proporciona buenos resultados en problemas de clasificación. Se propone una estrategia donde el modelo `InceptionResNetV2` toma la función de extractor de características, obteniendo así las características profundas que posteriormente podrán ser utilizadas en la clasificación de las imágenes. En segundo punto, se propone una red basada en el modelo `DenseNet`. Un modelo de menor tamaño y que permite obtener resultados similares a los de `InceptionResNetV2`. En este caso se trata de re-entrenar el clasificador y aplicando `fine tuning` con el fin de ajustar los parámetros del modelo.\n",
        "\n",
        "Dado que se parte de un conjunto de datos desbalanceado, es necesario aplicar algunos métodos de balanceo con el fin de obtener un modelo más robusto. Previamente, al entrenamiento de la red, se propone el uso de técnicas como la estratificación de datos o `data augmentation`. A su vez, para mejorar el proceso de entrenamiento se hace empleo de `mini-batches` y de optimizadores como el optimizador Adam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DLNUw-vuvhRx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow_addons.metrics import F1Score \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Keras imports\n",
        "from tensorflow.keras import optimizers, Input\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Concatenate, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.applications import InceptionResNetV2, DenseNet169 \n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-MWan3KCXmG",
        "outputId": "8f818470-36ed-4c52-f295-a07b81e27e54"
      },
      "outputs": [],
      "source": [
        "_isColab = False\n",
        "\n",
        "if _isColab:\n",
        "    from google.colab import drive\n",
        "\n",
        "    # Montamos el Google Drive en el directorio del proyecto y descomprimios el fichero con los datos\n",
        "    drive.mount('/content/drive')\n",
        "    !unzip -n '/content/drive/MyDrive/UIMP/Asignaturas/5-DeepLearning/Practicas/Final/Source/RT-BENE.zip' >> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrJHUIkdv9RO"
      },
      "source": [
        "## Preparar datos de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeLGe15ZLHjX"
      },
      "source": [
        "*   **Cargar el dataset en el colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sOwv5P20IHLv",
        "outputId": "aeaee0a6-2d91-4217-e64a-5afb112a8a4d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>blink_id</th>\n",
              "      <th>left_eye</th>\n",
              "      <th>right_eye</th>\n",
              "      <th>video</th>\n",
              "      <th>blink</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0_left_000001_rgb.png</td>\n",
              "      <td>0_right_000001_rgb.png</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0_left_000002_rgb.png</td>\n",
              "      <td>0_right_000002_rgb.png</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0_left_000003_rgb.png</td>\n",
              "      <td>0_right_000003_rgb.png</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0_left_000004_rgb.png</td>\n",
              "      <td>0_right_000004_rgb.png</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0_left_000005_rgb.png</td>\n",
              "      <td>0_right_000005_rgb.png</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   blink_id               left_eye               right_eye  video blink\n",
              "0         0  0_left_000001_rgb.png  0_right_000001_rgb.png      0     0\n",
              "1         1  0_left_000002_rgb.png  0_right_000002_rgb.png      0     0\n",
              "2         2  0_left_000003_rgb.png  0_right_000003_rgb.png      0     0\n",
              "3         3  0_left_000004_rgb.png  0_right_000004_rgb.png      0     0\n",
              "4         4  0_left_000005_rgb.png  0_right_000005_rgb.png      0     0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Especificamos los paths al directorio que contiene las imagenes y al fichero con las etiquetas\n",
        "data_path = 'RT-BENE/'\n",
        "\n",
        "if _isColab == False:\n",
        "    data_path = \"./../RT-BENE/\"\n",
        "\n",
        "imgs_path = data_path + \"images/\"\n",
        "labels_path = data_path + \"blinks.csv\"\n",
        "\n",
        "\n",
        "# Leemos el fichero CSV con las etiquetas\n",
        "labels = pd.read_csv(labels_path, dtype = {\"class\": \"category\"})\n",
        "\n",
        "# Supongamos que tienes un DataFrame llamado 'labels' con los nombres de las imágenes y las etiquetas 'blink'\n",
        "# Convertir la columna 'blink' a tipo string\n",
        "labels['blink'] = labels['blink'].astype(str)\n",
        "\n",
        "# Mostramos los primero elementos del dataset\n",
        "labels.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7VPECAbLCyN"
      },
      "source": [
        "- **Creamos las tres particiones de datos: entrenamiento, validación y test**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8FXPaZ16Ig0Y"
      },
      "outputs": [],
      "source": [
        "# Semilla para replicar los experimentos\n",
        "seed = 2023\n",
        "# Creamos las tres particiones de datos: entrenamiento, validación y test\n",
        "train_data, test_data = train_test_split(labels, test_size=0.2, random_state=seed)\n",
        "dev_data, test_data = train_test_split(test_data, test_size=0.5, random_state=seed)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xwZjOHteLfAo"
      },
      "source": [
        "En Keras no existen generadores por defecto que devuelvan dos imágenes. Necesitamos crear nuestro propio generador que devuelva a la vez las imágenes de ambos ojos y la etiqueta del frame. Para ello podemos crear dos generadores (uno para cada ojo) usando el método `flow_from_databrame` y combinarlos para crear el generador deseado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1tKJqLxZv836"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Columnas dataset\n",
        "left_eye_col = 'left_eye'\n",
        "right_eye_col = 'right_eye'\n",
        "y_col = 'blink'\n",
        "\n",
        "# Parámetros\n",
        "batch_size = 128\n",
        "img_width = 122 # minimo 75x75\n",
        "img_height = 75\n",
        "\n",
        "# Generador custom que devuelve las dos imagenes de ojos y el label del parpadeo\n",
        "def generator(dataframe):\n",
        "  left_eye_generator = datagen.flow_from_dataframe(dataframe=dataframe, \n",
        "                                                    directory = imgs_path, \n",
        "                                                    target_size =(img_width, img_height), \n",
        "                                                    x_col=left_eye_col, \n",
        "                                                    y_col=y_col, \n",
        "                                                    class_mode=\"binary\", \n",
        "                                                    seed=seed, \n",
        "                                                    batch_size=batch_size)\n",
        "  \n",
        "  right_eye_generator = datagen.flow_from_dataframe(dataframe=dataframe,\n",
        "                                                    directory = imgs_path,\n",
        "                                                    target_size =(img_width, img_height),\n",
        "                                                    x_col=right_eye_col,\n",
        "                                                    y_col=y_col,\n",
        "                                                    class_mode=\"binary\",\n",
        "                                                    seed=seed,\n",
        "                                                    batch_size=batch_size)\n",
        "  \n",
        "  while True:\n",
        "    left_eye = left_eye_generator.next()\n",
        "    left_eye_image = left_eye[0]\n",
        "    label = left_eye[1]\n",
        "    right_eye = right_eye_generator.next()\n",
        "    right_eye_image = right_eye[0]\n",
        "    yield [left_eye_image, right_eye_image], label\n",
        "\n",
        "# Llamada a la función generator\n",
        "train_generator = generator(train_data)\n",
        "dev_generator = generator(train_data)\n",
        "test_generator = generator(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\" Configuraciones previas \"\"\"\n",
        "\n",
        "# Definimos la métrica F1\n",
        "f1_score = F1Score(name=\"f1_score\", num_classes=1, threshold=0.5, average='macro')\n",
        "\n",
        "# Declaramos dos capas de Input\n",
        "input_shape = (img_width,img_height,3)\n",
        "\n",
        "input_image1 = Input(shape=input_shape)\n",
        "input_image2 = Input(shape=input_shape)\n",
        "\n",
        "# Definimos el ratio de aprendizaje\n",
        "lr = 1e-4"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trasnfer learing con IceptionResNetV2 como extractor de características"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes de entrenar el modelo, es importante decidir qué capas se actualizarán durante el entrenamiento. Si deseas aplicar fine-tuning en algunas de las capas del modelo base, primero puedes congelar todas las capas y luego descongelar las últimas capas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XZn-QbmRI8Q",
        "outputId": "8af6f2a2-8e83-494b-ba6e-b5fdc039aec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metal device set to: Apple M1 Ultra\n",
            "\n",
            "systemMemory: 64.00 GB\n",
            "maxCacheSize: 24.00 GB\n",
            "\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219055592/219055592 [==============================] - 12s 0us/step\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 122, 75, 3)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 122, 75, 3)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " inception_resnet_v2 (Functiona  (None, 1536)        54336736    ['input_1[0][0]',                \n",
            " l)                                                               'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 3072)         0           ['inception_resnet_v2[0][0]',    \n",
            "                                                                  'inception_resnet_v2[1][0]']    \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 1024)         3146752     ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 1024)         0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 512)          524800      ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 1)            513         ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 58,008,801\n",
            "Trainable params: 3,672,065\n",
            "Non-trainable params: 54,336,736\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Cargar InceptionResNetV2 pre-entrenado en ImageNet, sin la capa final\n",
        "base_model_incept = InceptionResNetV2(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "# Congelar todas las capas del modelo base para aplicar transfer learning\n",
        "for layer in base_model_incept.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Extraer las características utilizando InceptionResNetV2\n",
        "features_image1 = base_model_incept(input_image1)\n",
        "features_image2 = base_model_incept(input_image2)\n",
        "\n",
        "# Combinar las características de ambas imágenes\n",
        "merged_features = Concatenate()([features_image1, features_image2])\n",
        "\n",
        "# Añadir capas adicionales para la clasificación\n",
        "x = Dense(1024, activation='relu')(merged_features)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Crear el modelo\n",
        "model_incept = Model(inputs=[input_image1, input_image2], outputs=output)\n",
        "\n",
        "# Compilar el modelo\n",
        "model_incept.compile(optimizer=optimizers.Adam(learning_rate=lr), loss='binary_crossentropy', metrics=['accuracy', f1_score])\n",
        "\n",
        "# Imprimir un resumen del modelo\n",
        "model_incept.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB-EZ9e5SUiR",
        "outputId": "03c452e9-1865-4b61-e3ea-25efda3f0137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 85880 validated image filenames belonging to 2 classes.\n",
            "Found 85880 validated image filenames belonging to 2 classes.\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-05 14:32:12.879909: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "670/670 - 112s - loss: 0.0550 - accuracy: 0.9816 - val_loss: 0.0299 - val_accuracy: 0.9907 - 112s/epoch - 168ms/step\n",
            "Epoch 2/10\n",
            "670/670 - 93s - loss: 0.0304 - accuracy: 0.9898 - val_loss: 0.0219 - val_accuracy: 0.9924 - 93s/epoch - 138ms/step\n",
            "Epoch 3/10\n",
            "670/670 - 92s - loss: 0.0250 - accuracy: 0.9917 - val_loss: 0.0207 - val_accuracy: 0.9931 - 92s/epoch - 137ms/step\n",
            "Epoch 4/10\n",
            "670/670 - 92s - loss: 0.0215 - accuracy: 0.9928 - val_loss: 0.0202 - val_accuracy: 0.9934 - 92s/epoch - 138ms/step\n",
            "Epoch 5/10\n",
            "670/670 - 92s - loss: 0.0186 - accuracy: 0.9940 - val_loss: 0.0195 - val_accuracy: 0.9937 - 92s/epoch - 137ms/step\n",
            "Epoch 6/10\n",
            "670/670 - 91s - loss: 0.0167 - accuracy: 0.9944 - val_loss: 0.0160 - val_accuracy: 0.9942 - 91s/epoch - 136ms/step\n",
            "Epoch 7/10\n",
            "670/670 - 91s - loss: 0.0145 - accuracy: 0.9950 - val_loss: 0.0111 - val_accuracy: 0.9961 - 91s/epoch - 136ms/step\n",
            "Epoch 8/10\n",
            "670/670 - 97s - loss: 0.0140 - accuracy: 0.9952 - val_loss: 0.0079 - val_accuracy: 0.9973 - 97s/epoch - 145ms/step\n",
            "Epoch 9/10\n",
            "670/670 - 91s - loss: 0.0126 - accuracy: 0.9955 - val_loss: 0.0065 - val_accuracy: 0.9976 - 91s/epoch - 136ms/step\n",
            "Epoch 10/10\n",
            "670/670 - 92s - loss: 0.0107 - accuracy: 0.9963 - val_loss: 0.0076 - val_accuracy: 0.9972 - 92s/epoch - 137ms/step\n",
            "\n",
            "Found 85880 validated image filenames belonging to 2 classes.\n",
            "Found 85880 validated image filenames belonging to 2 classes.\n",
            "83/83 [==============================] - 10s 122ms/step - loss: 0.0073 - accuracy: 0.9977\n",
            "test_loss: 0.0073, test_acc: 0.9977\n"
          ]
        }
      ],
      "source": [
        "# Configuración del método 'Early stopping'\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Entrenamos el modelo con los datos preparados en el punto 2\n",
        "model_incept.fit(train_generator,\n",
        "          epochs=10,  # numero de epochs\n",
        "          verbose=2,  # muestra informacion del error al finalizar cada epoch\n",
        "          steps_per_epoch=len(train_data)/batch_size,\n",
        "          validation_data=train_generator,\n",
        "          validation_steps=len(dev_data)/batch_size,\n",
        "          callbacks=[early_stopping]) # Añadir el callback de 'early stoppoing'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Después del entrenamiento, evalúa el rendimiento del modelo en el conjunto de prueba:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Por ultimo, podemos evaluar el modelo en el conjunto de test\n",
        "print()\n",
        "test_loss, test_acc, test_f1_score = model_incept.evaluate(test_generator, steps=len(test_data) / batch_size, verbose=1)\n",
        "print(f\"Test_loss: {test_loss:.4f}, Test_acc: {test_acc:.4f}, Test_F1_Score: {test_f1_score:.4f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Además de analizar el error obtenido, podemos utilizarlo para hacer predicciones. Para ello utilizaremos el método `predict`, al que le suministremos los datos de entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "7TMOFXPXE3YE",
        "outputId": "9d1a7e44-97a9-4324-d79f-407c2a18dfe5"
      },
      "outputs": [],
      "source": [
        "# Obtenemos las predicciones para todos los ejemplos del conjunto de test\n",
        "predictions = model_incept.predict(test_generator, verbose=1)\n",
        "\n",
        "# Imprimimos la predicción obtenida para los dos primeros ejemplos\n",
        "# Los valores obtenidos representan las probabilidades para cada una de las 5 clases\n",
        "for i in range(0,2):\n",
        "  print(\"\\n Ejemplo\", i)\n",
        "  print(\"\\t Probabilidades para las 5 clases:\", predictions[i])\n",
        "  print(\"\\t Clase predicha: %i, Probabilidad: %.4f\" % (np.argmax([predictions[i]]), np.max(predictions[i])))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine Tuning con DenseNet"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Construir el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar el modelo DenseNet121 pre-entrenado en ImageNet sin incluir la capa de salida\n",
        "base_model_densenet = DenseNet169(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
        "\n",
        "# Congelar todas las capas del modelo base\n",
        "for layer in base_model_densenet.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Descongelar las últimas capas para aplicar fine-tuning\n",
        "# (en este ejemplo, descongelamos las últimas 50 capas)\n",
        "for layer in base_model_densenet.layers[-50:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Añadir una capa de GlobalAveragePooling2D\n",
        "x = base_model_densenet.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Añadir una capa Dense de salida con una función de activación sigmoide (clasificación binaria)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Crear el nuevo modelo\n",
        "model_dn = Model(inputs=base_model_densenet.input, outputs=output)\n",
        "\n",
        "# Compilar el modelo con la función de pérdida binary_crossentropy y la métrica de precisión\n",
        "model_dn.compile(optimizer=optimizers.Adam(learning_rate=lr), loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Entrenar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuración del método 'Early stopping'\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Entrenamos el modelo con los datos preparados en el punto 2\n",
        "model_incept.fit(train_generator,\n",
        "          epochs=10,  # numero de epochs\n",
        "          verbose=2,  # muestra informacion del error al finalizar cada epoch\n",
        "          steps_per_epoch=len(train_data)/batch_size,\n",
        "          validation_data=train_generator,\n",
        "          validation_steps=len(dev_data)/batch_size,\n",
        "          callbacks=[early_stopping]) # Añadir el callback de 'early stoppoing'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Después del entrenamiento, evalúa el rendimiento del modelo en el conjunto de prueba:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Por ultimo, podemos evaluar el modelo en el conjunto de test\n",
        "print()\n",
        "test_loss, test_acc, test_f1_score = model_dn.evaluate(test_generator, steps=len(test_data) / batch_size, verbose=1)\n",
        "print(f\"Test_loss: {test_loss:.4f}, Test_acc: {test_acc:.4f}, Test_F1_Score: {test_f1_score:.4f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Además de analizar el error obtenido, podemos utilizarlo para hacer predicciones. Para ello utilizaremos el método `predict`, al que le suministremos los datos de entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtenemos las predicciones para todos los ejemplos del conjunto de test\n",
        "predictions = model_incept.predict(test_generator, verbose=1)\n",
        "\n",
        "# Imprimimos la predicción obtenida para los dos primeros ejemplos\n",
        "# Los valores obtenidos representan las probabilidades para cada una de las 5 clases\n",
        "for i in range(0,2):\n",
        "  print(\"\\n Ejemplo\", i)\n",
        "  print(\"\\t Probabilidades para las 5 clases:\", predictions[i])\n",
        "  print(\"\\t Clase predicha: %i, Probabilidad: %.4f\" % (np.argmax([predictions[i]]), np.max(predictions[i])))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPYRKocSK9HLJdrEdQMa+Yt",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
